# -*- coding: utf-8 -*-
"""musma_nlp_0.62.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S18nkNRcBzrEa05BNL_JaotggI67RGJy

# 데이터셋
"""

import pandas as pd

# 엑셀 파일 로드
file_path = '/content/sample_data/musma+manual+manual_crawling.xlsx'
data = pd.read_excel(file_path)

# 데이터의 처음 몇 행을 확인
data.head()
data.shape

"""### 데이터 전처리"""

# 데이터셋을 재구성하기 위해 각 카테고리 별로 데이터를 하나의 리스트에 저장합니다.
# 각 뉴스 제목에 해당 카테고리의 레이블을 부여합니다.

# 카테고리를 레이블로 매핑
categories = data.columns
label_map = {category: i for i, category in enumerate(categories)}

# (뉴스 제목, 레이블) 형식의 리스트 생성
news_data = []
for category in categories:
    news_titles = data[category].dropna().tolist()  # NaN 값 제거
    news_data.extend([(title, label_map[category]) for title in news_titles])

# 데이터셋을 DataFrame으로 변환
news_df = pd.DataFrame(news_data, columns=['news_title', 'label'])

news_df.head(), label_map

!pip install transformers
!pip install torch

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split
from sklearn.model_selection import train_test_split
from transformers import AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm
import numpy as np

tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=4)

# 뉴스 제목을 토큰화합니다.
input_ids = []
attention_masks = []

for news in news_df['news_title']:
    encoded_dict = tokenizer.encode_plus(
                        news,                      # 뉴스 제목을 인코딩
                        add_special_tokens = True, # 특수 토큰 추가
                        max_length = 64,           # 문장의 최대 토큰 수 설정
                        pad_to_max_length = True,  # 짧은 문장은 패딩으로 채움
                        return_attention_mask = True,   # 어텐션 마스크 반환
                        return_tensors = 'pt',     # 파이토치 텐서로 반환
                   )

    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

# 토큰화된 데이터를 파이토치 텐서로 변환합니다.
input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(news_df['label'].values)

# 훈련 세트와 검증 세트로 분할합니다.
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.2)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.2)

# 데이터 로더를 생성합니다.
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)

validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
validation_sampler = SequentialSampler(validation_data)
validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)

# KoBERT 토크나이저와 모델을 로드
tokenizer = BertTokenizer.from_pretrained('monologg/kobert')
model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=4)

# GPU 사용 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 학습 코드
epochs = 100
loss_values = []

# 옵티마이저 설정
optimizer = AdamW(model.parameters(),
                  lr = 2e-5, # 학습률
                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 매우 작은 숫자
                )

# 총 훈련 스텝은 에폭 수와 배치 사이즈에 따라 결정
total_steps = len(train_dataloader) * epochs

# 학습률 스케줄러 설정
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0, # Warmup 스텝
                                            num_training_steps = total_steps)

from sklearn.metrics import f1_score
import matplotlib.pyplot as plt

# 손실과 정확도, F1 점수를 기록할 리스트 초기화
loss_values = []
validation_accuracy = []
validation_f1_scores = []

for epoch_i in range(0, epochs):
    # 에폭당 학습
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
    total_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        # 배치를 GPU에 로드
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        # 그래디언트 초기화
        model.zero_grad()

        # 순전파
        outputs = model(b_input_ids,
                        token_type_ids=None,
                        attention_mask=b_input_mask,
                        labels=b_labels)

        # 손실 얻기
        loss = outputs.loss
        total_loss += loss.item()

        # 역전파
        loss.backward()

        # 그래디언트 클리핑
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # 가중치 업데이트
        optimizer.step()

        # 학습률 감소
        scheduler.step()

    # 평균 손실 계산
    avg_train_loss = total_loss / len(train_dataloader)
    loss_values.append(avg_train_loss)
    print("  Average training loss: {0:.2f}".format(avg_train_loss))

    # 검증
    print("  Running Validation...")
    model.eval()
    # 변수 초기화
    total_eval_accuracy = 0
    total_eval_f1 = 0
    total_eval_examples = 0

    for batch in validation_dataloader:
        # 배치를 GPU에 로드
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        # 그래디언트 계산을 하지 않도록 설정
        with torch.no_grad():
            # 순전파 진행
            outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask)

        # 로그트와 레이블을 CPU로 이동
        logits = outputs.logits
        logits = logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()

        # 출력 로그트의 가장 높은 점수를 받은 클래스를 예측값으로 선택
        preds = np.argmax(logits, axis=1)

        # 정확도 계산
        total_eval_accuracy += (preds == label_ids).mean()

        # F1 점수 계산
        total_eval_f1 += f1_score(label_ids, preds, average='weighted')

        # 처리한 예제 수 업데이트
        total_eval_examples += b_input_ids.size(0)

    # 평균 정확도 계산 및 기록
    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    validation_accuracy.append(avg_val_accuracy)
    print("  Validation Accuracy: {0:.2f}".format(avg_val_accuracy))

    # 평균 F1 점수 계산 및 기록
    avg_val_f1 = total_eval_f1 / len(validation_dataloader)
    validation_f1_scores.append(avg_val_f1)
    print("  Validation F1 Score: {0:.2f}".format(avg_val_f1))

# 전체 학습 및 검증 과정에서의 평균 손실 및 정확도 계산
avg_loss_over_epochs = sum(loss_values) / len(loss_values)
avg_accuracy_over_epochs = sum(validation_accuracy) / len(validation_accuracy)
avg_f1_over_epochs = sum(validation_f1_scores) / len(validation_f1_scores)

print('================================')
print(f"Final Average Training Loss: {avg_loss_over_epochs:.2f}")
print(f"Final Average Validation Accuracy: {avg_accuracy_over_epochs:.2f}")
print(f"Final Average Validation F1 Score: {avg_f1_over_epochs:.2f}")
print("Training complete!")

# 모델 저장
# model.save_pretrained('/content/drive/MyDrive/Model')
# tokenizer.save_pretrained('/content/drive/MyDrive/Model')

"""### 시각화"""

plt.figure(figsize=(12, 6))
# 학습 손실 그래프
plt.subplot(1, 3, 1)
plt.plot(loss_values, 'b-o')
plt.title("Training Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")

# 검증 정확도 그래프
plt.subplot(1, 3, 2)
plt.plot(validation_accuracy, 'r-o')
plt.title("Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")

# 검증 F1 점수 그래프
plt.subplot(1, 3, 3)
plt.plot(validation_f1_scores, 'g-o')
plt.title("Validation F1 Score")
plt.xlabel("Epoch")
plt.ylabel("F1 Score")

plt.tight_layout()
plt.show()

print("Training complete!")